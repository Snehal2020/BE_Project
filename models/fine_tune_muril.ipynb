{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (AutoModelForSequenceClassification, AutoTokenizer,\n",
    "                          DataCollatorWithPadding, Trainer, TrainingArguments)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- 1Ô∏è‚É£ Scrape News from RSS Feeds & NewsAPI -----------------\n",
    "RSS_FEEDS = {\n",
    "    \"finance_en\": \"https://rss.cnn.com/rss/money_news_international.rss\",\n",
    "    \"finance_hi\": \"https://www.bbc.com/hindi/index.xml\",\n",
    "    \"healthcare_en\": \"https://www.who.int/rss-feeds/news-english.xml\",\n",
    "    \"healthcare_hi\": \"https://www.bbc.com/hindi/science-and-environment/index.xml\",\n",
    "    \"education_en\": \"https://www.theguardian.com/education/rss\",\n",
    "    \"education_hi\": \"https://www.bbc.com/hindi/india/index.xml\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_news_from_rss():\n",
    "    \"\"\"Fetch news from RSS feeds\"\"\"\n",
    "    news_data = []\n",
    "    for category, url in RSS_FEEDS.items():\n",
    "        try:\n",
    "            feed = feedparser.parse(url)\n",
    "            for entry in feed.entries[:10]:  # Fetch only 10 articles per category\n",
    "                title = entry.get(\"title\", \"\")\n",
    "                description = entry.get(\"description\", \"\")\n",
    "                news_data.append({\"category\": category, \"title\": title, \"content\": description})\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error fetching RSS feed {category}: {e}\")\n",
    "    return news_data\n",
    "\n",
    "def fetch_news_from_newsapi():\n",
    "    \"\"\"Fetch news from NewsAPI\"\"\"\n",
    "    API_KEY = \"033302b4ad3c4ca1bc664e1c784bb622\"  # Replace with your NewsAPI key\n",
    "    NEWSAPI_URL = \"https://newsapi.org/v2/top-headlines\"\n",
    "    categories = [\"business\", \"health\", \"education\"]\n",
    "\n",
    "    news_data = []\n",
    "    for category in categories:\n",
    "        try:\n",
    "            params = {\"category\": category, \"language\": \"en\", \"apiKey\": API_KEY}\n",
    "            response = requests.get(NEWSAPI_URL, params=params)\n",
    "            if response.status_code == 200:\n",
    "                articles = response.json().get(\"articles\", [])\n",
    "                for article in articles[:10]:  # Fetch only 10 articles per category\n",
    "                    title = article.get(\"title\", \"\")\n",
    "                    description = article.get(\"description\", \"\")\n",
    "                    news_data.append({\"category\": category, \"title\": title, \"content\": description})\n",
    "            else:\n",
    "                print(f\"‚ùå API error for {category}: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error fetching NewsAPI data: {e}\")\n",
    "    return news_data\n",
    "\n",
    "# Fetch news\n",
    "rss_news = fetch_news_from_rss()\n",
    "api_news = fetch_news_from_newsapi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ News fetched successfully! Total articles: 60\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(rss_news + api_news)\n",
    "df.drop_duplicates(subset=[\"title\"], inplace=True)  # Remove duplicate news\n",
    "print(f\"‚úÖ News fetched successfully! Total articles: {len(df)}\")\n",
    "\n",
    "# ----------------- 2Ô∏è‚É£ Preprocessing -----------------\n",
    "def clean_text(text):\n",
    "    \"\"\"Cleans text by removing HTML tags, URLs, special characters, and extra spaces\"\"\"\n",
    "    if pd.isna(text):  # Handle NaN values\n",
    "        return \"\"\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()  # Remove HTML tags\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Remove special characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "df[\"cleaned_title\"] = df[\"title\"].apply(clean_text)\n",
    "df[\"cleaned_content\"] = df[\"content\"].apply(clean_text)\n",
    "\n",
    "# Assigning labels randomly for now (replace with actual labels if available)\n",
    "df[\"label\"] = df[\"cleaned_title\"].apply(lambda x: random.choice([0, 1, 2]))  # 0: Negative, 1: Neutral, 2: Positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616b9d710f7645f1ab60d8b29159194c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/48 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad5a52599ea452a8fecab48bf611eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a39bab097e4508a626d4fcce02104e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----------------- 3Ô∏è‚É£ Fine-tune MuRIL for Sentiment Analysis -----------------\n",
    "MODEL_NAME = \"google/muril-base-cased\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)  # 3 labels: Negative, Neutral, Positive\n",
    "\n",
    "# Convert DataFrame to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Split into train and validation sets\n",
    "train_test = dataset.train_test_split(test_size=0.2)\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_test[\"train\"],\n",
    "    \"validation\": train_test[\"test\"]\n",
    "})\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"cleaned_title\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n",
    "\n",
    "# Data collator for padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "#from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()  # üöÄ Start fine-tuning\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./muril_sentiment_model\")\n",
    "tokenizer.save_pretrained(\"./muril_sentiment_model\")\n",
    "print(\"‚úÖ MuRIL fine-tuning completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix)\n",
    "\n",
    "\n",
    "# ----------------- Evaluate Model Performance -----------------\n",
    "def evaluate_model(model, dataset):\n",
    "    \"\"\"Evaluates the model on the validation dataset\"\"\"\n",
    "    all_predictions, all_labels = [], []\n",
    "    \n",
    "    for example in dataset:\n",
    "        text = example[\"cleaned_title\"]\n",
    "        label = example[\"label\"]\n",
    "        \n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        predicted_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "        \n",
    "        # Store predictions & actual labels\n",
    "        all_predictions.append(predicted_label)\n",
    "        all_labels.append(label)\n",
    "\n",
    "    return np.array(all_predictions), np.array(all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model predictions\n",
    "y_pred, y_true = evaluate_model(model, tokenized_datasets[\"validation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- Calculate Metrics -----------------\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"‚úÖ Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Precision, Recall, F1-score\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(\"\\nüåÄ Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
